{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "daf72fb4-2c80-4b14-b7db-ca2cc8ac9a33",
    "_uuid": "fc17850c-2548-4ce9-9146-8110e916a233",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "      <th>Cultivar 1</th>\n",
       "      <th>Cultivar 2</th>\n",
       "      <th>Cultivar 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.518613</td>\n",
       "      <td>-0.562250</td>\n",
       "      <td>0.232053</td>\n",
       "      <td>-1.169593</td>\n",
       "      <td>1.913905</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>1.034819</td>\n",
       "      <td>-0.659563</td>\n",
       "      <td>1.224884</td>\n",
       "      <td>0.251717</td>\n",
       "      <td>0.362177</td>\n",
       "      <td>1.847920</td>\n",
       "      <td>1.013009</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.246290</td>\n",
       "      <td>-0.499413</td>\n",
       "      <td>-0.827996</td>\n",
       "      <td>-2.490847</td>\n",
       "      <td>0.018145</td>\n",
       "      <td>0.568648</td>\n",
       "      <td>0.733629</td>\n",
       "      <td>-0.820719</td>\n",
       "      <td>-0.544721</td>\n",
       "      <td>-0.293321</td>\n",
       "      <td>0.406051</td>\n",
       "      <td>1.113449</td>\n",
       "      <td>0.965242</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.196879</td>\n",
       "      <td>0.021231</td>\n",
       "      <td>1.109334</td>\n",
       "      <td>-0.268738</td>\n",
       "      <td>0.088358</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>1.215533</td>\n",
       "      <td>-0.498407</td>\n",
       "      <td>2.135968</td>\n",
       "      <td>0.269020</td>\n",
       "      <td>0.318304</td>\n",
       "      <td>0.788587</td>\n",
       "      <td>1.395148</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.691550</td>\n",
       "      <td>-0.346811</td>\n",
       "      <td>0.487926</td>\n",
       "      <td>-0.809251</td>\n",
       "      <td>0.930918</td>\n",
       "      <td>2.491446</td>\n",
       "      <td>1.466525</td>\n",
       "      <td>-0.981875</td>\n",
       "      <td>1.032155</td>\n",
       "      <td>1.186068</td>\n",
       "      <td>-0.427544</td>\n",
       "      <td>1.184071</td>\n",
       "      <td>2.334574</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.295700</td>\n",
       "      <td>0.227694</td>\n",
       "      <td>1.840403</td>\n",
       "      <td>0.451946</td>\n",
       "      <td>1.281985</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>0.663351</td>\n",
       "      <td>0.226796</td>\n",
       "      <td>0.401404</td>\n",
       "      <td>-0.319276</td>\n",
       "      <td>0.362177</td>\n",
       "      <td>0.449601</td>\n",
       "      <td>-0.037874</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Alcohol  Malic acid       Ash  Alcalinity of ash  Magnesium  \\\n",
       "0  1.518613   -0.562250  0.232053          -1.169593   1.913905   \n",
       "1  0.246290   -0.499413 -0.827996          -2.490847   0.018145   \n",
       "2  0.196879    0.021231  1.109334          -0.268738   0.088358   \n",
       "3  1.691550   -0.346811  0.487926          -0.809251   0.930918   \n",
       "4  0.295700    0.227694  1.840403           0.451946   1.281985   \n",
       "\n",
       "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "0       0.808997    1.034819             -0.659563         1.224884   \n",
       "1       0.568648    0.733629             -0.820719        -0.544721   \n",
       "2       0.808997    1.215533             -0.498407         2.135968   \n",
       "3       2.491446    1.466525             -0.981875         1.032155   \n",
       "4       0.808997    0.663351              0.226796         0.401404   \n",
       "\n",
       "   Color intensity       Hue  OD280/OD315 of diluted wines   Proline  \\\n",
       "0         0.251717  0.362177                      1.847920  1.013009   \n",
       "1        -0.293321  0.406051                      1.113449  0.965242   \n",
       "2         0.269020  0.318304                      0.788587  1.395148   \n",
       "3         1.186068 -0.427544                      1.184071  2.334574   \n",
       "4        -0.319276  0.362177                      0.449601 -0.037874   \n",
       "\n",
       "   Cultivar 1  Cultivar 2  Cultivar 3  \n",
       "0           1           0           0  \n",
       "1           1           0           0  \n",
       "2           1           0           0  \n",
       "3           1           0           0  \n",
       "4           1           0           0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('W1data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "550c94d3-a7d6-49de-a7bc-3327cf2a7b8b",
    "_uuid": "4da2ce95-3ea9-411d-9100-aa906af67773"
   },
   "outputs": [],
   "source": [
    "# Get the wine labels\n",
    "y = df[['Cultivar 1', 'Cultivar 2', 'Cultivar 3']].values\n",
    "\n",
    "# Get inputs; we define our x and y here.\n",
    "X = df.drop(['Cultivar 1', 'Cultivar 2', 'Cultivar 3'], axis = 1)\n",
    "X.shape, y.shape # Print shapes just to check\n",
    "X = X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "1f665b7c-78d6-4093-8a0a-1d2d90dc19e2",
    "_uuid": "ea2d411f-043f-43bd-b22c-6a0c077875b1"
   },
   "outputs": [],
   "source": [
    "#First we are importing all the libraries\n",
    "\n",
    "# Package imports\n",
    "# Matplotlib is a matlab like plotting library\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# SciKitLearn is a useful machine learning utilities library\n",
    "import sklearn\n",
    "# The sklearn dataset module helps generating datasets\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "c0acd657-fc8a-4d88-a2fb-f794815d5e88",
    "_uuid": "244b2313-0638-4581-a531-0b9b58caba4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0 : 1.4504813381979211\n",
      "Accuracy after iteration 0 : 25.280898876404496 %\n",
      "Loss after iteration 100 : 0.5255210272876468\n",
      "Accuracy after iteration 100 : 80.89887640449437 %\n",
      "Loss after iteration 200 : 0.3964222056044325\n",
      "Accuracy after iteration 200 : 86.51685393258427 %\n",
      "Loss after iteration 300 : 0.34694059691857543\n",
      "Accuracy after iteration 300 : 87.64044943820225 %\n",
      "Loss after iteration 400 : 0.32062505328047924\n",
      "Accuracy after iteration 400 : 89.32584269662921 %\n",
      "Loss after iteration 500 : 0.30057660926569424\n",
      "Accuracy after iteration 500 : 89.32584269662921 %\n",
      "Loss after iteration 600 : 0.2775754239020053\n",
      "Accuracy after iteration 600 : 89.8876404494382 %\n",
      "Loss after iteration 700 : 0.25803463679942457\n",
      "Accuracy after iteration 700 : 90.4494382022472 %\n",
      "Loss after iteration 800 : 0.2451896091466233\n",
      "Accuracy after iteration 800 : 90.4494382022472 %\n",
      "Loss after iteration 900 : 0.23453472996993888\n",
      "Accuracy after iteration 900 : 90.4494382022472 %\n",
      "Loss after iteration 1000 : 0.224901853756763\n",
      "Accuracy after iteration 1000 : 90.4494382022472 %\n",
      "Loss after iteration 1100 : 0.21649115993849932\n",
      "Accuracy after iteration 1100 : 92.13483146067416 %\n",
      "Loss after iteration 1200 : 0.20897333560828457\n",
      "Accuracy after iteration 1200 : 93.25842696629213 %\n",
      "Loss after iteration 1300 : 0.20186399701140648\n",
      "Accuracy after iteration 1300 : 93.25842696629213 %\n",
      "Loss after iteration 1400 : 0.18022550179248026\n",
      "Accuracy after iteration 1400 : 93.25842696629213 %\n",
      "Loss after iteration 1500 : 0.16690182286613958\n",
      "Accuracy after iteration 1500 : 93.82022471910112 %\n",
      "Loss after iteration 1600 : 0.15681341901851126\n",
      "Accuracy after iteration 1600 : 94.3820224719101 %\n",
      "Loss after iteration 1700 : 0.14891431157061535\n",
      "Accuracy after iteration 1700 : 94.9438202247191 %\n",
      "Loss after iteration 1800 : 0.14240653665707972\n",
      "Accuracy after iteration 1800 : 94.9438202247191 %\n",
      "Loss after iteration 1900 : 0.13578456173744088\n",
      "Accuracy after iteration 1900 : 95.50561797752809 %\n",
      "Loss after iteration 2000 : 0.1319146672331526\n",
      "Accuracy after iteration 2000 : 95.50561797752809 %\n",
      "Loss after iteration 2100 : 0.1290070757552617\n",
      "Accuracy after iteration 2100 : 95.50561797752809 %\n",
      "Loss after iteration 2200 : 0.1266137159634668\n",
      "Accuracy after iteration 2200 : 95.50561797752809 %\n",
      "Loss after iteration 2300 : 0.12454049447039095\n",
      "Accuracy after iteration 2300 : 95.50561797752809 %\n",
      "Loss after iteration 2400 : 0.12268163837704688\n",
      "Accuracy after iteration 2400 : 95.50561797752809 %\n",
      "Loss after iteration 2500 : 0.12096337880182337\n",
      "Accuracy after iteration 2500 : 95.50561797752809 %\n",
      "Loss after iteration 2600 : 0.11931607604440037\n",
      "Accuracy after iteration 2600 : 95.50561797752809 %\n",
      "Loss after iteration 2700 : 0.11764906434462091\n",
      "Accuracy after iteration 2700 : 96.06741573033707 %\n",
      "Loss after iteration 2800 : 0.115786808903355\n",
      "Accuracy after iteration 2800 : 96.06741573033707 %\n",
      "Loss after iteration 2900 : 0.11312910792526902\n",
      "Accuracy after iteration 2900 : 96.06741573033707 %\n",
      "Loss after iteration 3000 : 0.10512216026989832\n",
      "Accuracy after iteration 3000 : 96.06741573033707 %\n",
      "Loss after iteration 3100 : 0.08528325019384479\n",
      "Accuracy after iteration 3100 : 97.19101123595506 %\n",
      "Loss after iteration 3200 : 0.07310937002597404\n",
      "Accuracy after iteration 3200 : 98.31460674157303 %\n",
      "Loss after iteration 3300 : 0.07094849409554892\n",
      "Accuracy after iteration 3300 : 98.31460674157303 %\n",
      "Loss after iteration 3400 : 0.06647041651883527\n",
      "Accuracy after iteration 3400 : 98.31460674157303 %\n",
      "Loss after iteration 3500 : 0.06349694100556137\n",
      "Accuracy after iteration 3500 : 98.31460674157303 %\n",
      "Loss after iteration 3600 : 0.06209091813348396\n",
      "Accuracy after iteration 3600 : 98.87640449438202 %\n",
      "Loss after iteration 3700 : 0.060987233032883656\n",
      "Accuracy after iteration 3700 : 98.87640449438202 %\n",
      "Loss after iteration 3800 : 0.059896142131366584\n",
      "Accuracy after iteration 3800 : 98.87640449438202 %\n",
      "Loss after iteration 3900 : 0.05868449560544277\n",
      "Accuracy after iteration 3900 : 98.87640449438202 %\n",
      "Loss after iteration 4000 : 0.05727076686257899\n",
      "Accuracy after iteration 4000 : 98.87640449438202 %\n",
      "Loss after iteration 4100 : 0.0554444294154591\n",
      "Accuracy after iteration 4100 : 98.87640449438202 %\n",
      "Loss after iteration 4200 : 0.051882487945802656\n",
      "Accuracy after iteration 4200 : 98.87640449438202 %\n",
      "Loss after iteration 4300 : 0.043725816711018155\n",
      "Accuracy after iteration 4300 : 99.43820224719101 %\n",
      "Loss after iteration 4400 : 0.038547256153144846\n",
      "Accuracy after iteration 4400 : 99.43820224719101 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5ee75b9340>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we define all our functions\n",
    "\n",
    "def softmax(z):\n",
    "    #Calculate exponent term first\n",
    "    exp_scores = np.exp(z)\n",
    "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "def softmax_loss(y,y_hat):\n",
    "    # Clipping value\n",
    "    minval = 0.000000000001\n",
    "    # Number of samples\n",
    "    m = y.shape[0]\n",
    "    # Loss formula, note that np.sum sums up the entire matrix and therefore does the job of two sums from the formula\n",
    "    loss = -1/m * np.sum(y * np.log(y_hat.clip(min=minval)))\n",
    "    return loss\n",
    "\n",
    "def loss_derivative(y,y_hat):\n",
    "    return (y_hat-y)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return (1 - np.power(x, 2))\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "#FORWARD PROPAGATION\n",
    "# This is the forward propagation function\n",
    "def forward_prop(model,a0):\n",
    "\n",
    "    \n",
    "    # Load parameters from model\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'],model['b3']\n",
    "    \n",
    "    # Do the first Linear step \n",
    "    # Z1 is the input layer x times the dot product of the weights + our bias b\n",
    "    z1 = a0.dot(W1) + b1\n",
    "    \n",
    "    # Put it through the first activation function\n",
    "    a1 = np.tanh(z1)\n",
    "    \n",
    "    # Second linear step\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    \n",
    "    # Second activation function\n",
    "    a2 = np.tanh(z2)\n",
    "    \n",
    "    #Third linear step\n",
    "    z3 = a2.dot(W3) + b3\n",
    "    \n",
    "    #For the Third linear activation function we use the softmax function, either the sigmoid of softmax should be used for the last layer\n",
    "    a3 = softmax(z3)\n",
    "    \n",
    "    #Store all results in these values\n",
    "    cache = {'a0':a0,'z1':z1,'a1':a1,'z2':z2,'a2':a2,'a3':a3,'z3':z3}\n",
    "    return cache\n",
    "\n",
    "# This is the backward propagation function\n",
    "def backward_prop(model,cache,y):\n",
    "\n",
    "    # Load parameters from model\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'],model['W3'],model['b3']\n",
    "    \n",
    "    # Load forward propagation results\n",
    "    a0,a1, a2,a3 = cache['a0'],cache['a1'],cache['a2'],cache['a3']\n",
    "    \n",
    "    # Get number of samples\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    # Calculate loss derivative with respect to output\n",
    "    dz3 = loss_derivative(y=y,y_hat=a3)\n",
    "\n",
    "    # Calculate loss derivative with respect to second layer weights\n",
    "    dW3 = 1/m*(a2.T).dot(dz3) #dW2 = 1/m*(a1.T).dot(dz2) \n",
    "    \n",
    "    # Calculate loss derivative with respect to second layer bias\n",
    "    db3 = 1/m*np.sum(dz3, axis=0)\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer\n",
    "    dz2 = np.multiply(dz3.dot(W3.T) ,tanh_derivative(a2))\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer weights\n",
    "    dW2 = 1/m*np.dot(a1.T, dz2)\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer bias\n",
    "    db2 = 1/m*np.sum(dz2, axis=0)\n",
    "    \n",
    "    dz1 = np.multiply(dz2.dot(W2.T),tanh_derivative(a1))\n",
    "    \n",
    "    dW1 = 1/m*np.dot(a0.T,dz1)\n",
    "    \n",
    "    db1 = 1/m*np.sum(dz1,axis=0)\n",
    "    \n",
    "    # Store gradients\n",
    "    grads = {'dW3':dW3, 'db3':db3, 'dW2':dW2,'db2':db2,'dW1':dW1,'db1':db1}\n",
    "    return grads\n",
    "\n",
    "#TRAINING PHASE\n",
    "def initialize_parameters(nn_input_dim,nn_hdim,nn_output_dim):\n",
    "    # First layer weights\n",
    "    W1 = 2 *np.random.randn(nn_input_dim, nn_hdim) - 1\n",
    "    \n",
    "    # First layer bias\n",
    "    b1 = np.zeros((1, nn_hdim))\n",
    "    \n",
    "    # Second layer weights\n",
    "    W2 = 2 * np.random.randn(nn_hdim, nn_hdim) - 1\n",
    "    \n",
    "    # Second layer bias\n",
    "    b2 = np.zeros((1, nn_hdim))\n",
    "    W3 = 2 * np.random.rand(nn_hdim, nn_output_dim) - 1\n",
    "    b3 = np.zeros((1,nn_output_dim))\n",
    "    \n",
    "    \n",
    "    # Package and return model\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,'W3':W3,'b3':b3}\n",
    "    return model\n",
    "\n",
    "def update_parameters(model,grads,learning_rate):\n",
    "    # Load parameters\n",
    "    W1, b1, W2, b2,b3,W3 = model['W1'], model['b1'], model['W2'], model['b2'],model['b3'],model[\"W3\"]\n",
    "    \n",
    "    # Update parameters\n",
    "    W1 -= learning_rate * grads['dW1']\n",
    "    b1 -= learning_rate * grads['db1']\n",
    "    W2 -= learning_rate * grads['dW2']\n",
    "    b2 -= learning_rate * grads['db2']\n",
    "    W3 -= learning_rate * grads['dW3']\n",
    "    b3 -= learning_rate * grads['db3']\n",
    "    \n",
    "    # Store and return parameters\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3':W3,'b3':b3}\n",
    "    return model\n",
    "\n",
    "def predict(model, x):\n",
    "    # Do forward pass\n",
    "    c = forward_prop(model,x)\n",
    "    #get y_hat\n",
    "    y_hat = np.argmax(c['a3'], axis=1)\n",
    "    return y_hat\n",
    "\n",
    "def calc_accuracy(model,x,y):\n",
    "    # Get total number of examples\n",
    "    m = y.shape[0]\n",
    "    # Do a prediction with the model\n",
    "    pred = predict(model,x)\n",
    "    # Ensure prediction and truth vector y have the same shape\n",
    "    pred = pred.reshape(y.shape)\n",
    "    # Calculate the number of wrong examples\n",
    "    error = np.sum(np.abs(pred-y))\n",
    "    # Calculate accuracy\n",
    "    return (m - error)/m * 100\n",
    "losses = []\n",
    "\n",
    "def train(model,X_,y_,learning_rate, epochs=20000, print_loss=False):\n",
    "    # Gradient descent. Loop over epochs\n",
    "    for i in range(0, epochs):\n",
    "\n",
    "        # Forward propagation\n",
    "        cache = forward_prop(model,X_)\n",
    "        #a1, probs = cache['a1'],cache['a2']\n",
    "        # Backpropagation\n",
    "        \n",
    "        grads = backward_prop(model,cache,y_)\n",
    "        # Gradient descent parameter update\n",
    "        # Assign new parameters to the model\n",
    "        model = update_parameters(model=model,grads=grads,learning_rate=learning_rate)\n",
    "    \n",
    "        # Pring loss & accuracy every 100 iterations\n",
    "        if print_loss and i % 100 == 0:\n",
    "            a3 = cache['a3']\n",
    "            print('Loss after iteration',i,':',softmax_loss(y_,a3))\n",
    "            y_hat = predict(model,X_)\n",
    "            y_true = y_.argmax(axis=1)\n",
    "            \n",
    "            print('Accuracy after iteration',i,':',accuracy_score(y_pred=y_hat,y_true=y_true)*100,'%')\n",
    "            losses.append(accuracy_score(y_pred=y_hat,y_true=y_true)*100)\n",
    "    return model\n",
    "\n",
    "\n",
    "# This is what we return at the end\n",
    "model = initialize_parameters(nn_input_dim=13, nn_hdim= 5, nn_output_dim= 3)\n",
    "model = train(model,X,y,learning_rate=0.07,epochs=4500,print_loss=True)\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
